## 可微可塑性：一种学会学习的新方法


作为 Uber 机器学习系统基础的神经网络，在解决包括图像识别、语言理解和博弈论在内的复杂问题方面被证明是非常成功的。然而，网络通常通过 梯度下降 训练到一个终止点，根据多次试验中的网络表现不断调整网络连接。一旦训练完成，网络就已经固定，连接不再改变；因此，除了以后的再训练（又需要很多样本），实际上网络在训练结束时就停止学习。

相比之下，生物大脑表现出的 可塑性 —— 即在整个生命中，神经元之间连接持续不断地自主变化的能力，使动物能够从持续的经验中快速有效地学习。大脑中不同区域和连接的可塑性水平是通过数百万年的进化而进行微调的结果，以便在动物的一生中进行有效地学习。由此产生的持续学习能力可以让动物只需很少的额外信息（additional data）就能适应变化或不可预测的环境。我们可以很快地记住以前从未见过的场景，或者在完全陌生的情况下从几次试验中获得新的知识。

为了给我们的人工智能体提供类似的能力，Uber 人工智能实验室开发了 一种称为可微可塑性的新方法 让我们通过梯度下降训练可塑的连接行为，以便他们可以帮助以前训练的网络适应未来的环境。虽然演化这种可塑性神经网络是 进化计算长期研究的领域。据我们所知，这里介绍的工作首次表明可以通过梯度下降优化可塑性网络。因为最近人工智能领域的重大突破是以基于梯度的方法为基础的（包括 图像识别、机器翻译 和 对弈）。使可塑性网络适应梯度下降训练可能会极大扩展这两种方法的力量。

